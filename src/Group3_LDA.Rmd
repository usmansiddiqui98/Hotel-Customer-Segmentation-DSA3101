---
title: "DSA3101 project 1"
output: html_document
---

Loading of libraries
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
devtools::install_github("cpsievert/LDAvis")
devtools::install_github("cpsievert/LDAvisData")
library(lda)
library(LDAvis)
library(tidyverse)
library(tm)
library(stringr)
library(qdap)
library(textstem)
library(wordcloud)
library(RColorBrewer)
library(Rcpp)
library(tm)
library(ldatuning)
```

## Load in data and find out best K value (number of topics)
```{r}
data <- read_csv("dataset_sentiment_analysis.csv")
filter_data <- data %>%
    filter(!review_text == "There are no comments available for this review") %>%
  select(review_text, Cluster)

corpus <- Corpus(VectorSource(filter_data$review_text))
termdocmatrix <- TermDocumentMatrix(corpus)

result <- FindTopicsNumber(
  termdocmatrix,
  topics = seq(2,10, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 3101),
  mc.cores = NA,
  return_models = FALSE,
  verbose = FALSE,
  libpath = NULL
)

FindTopicsNumber_plot(result)
```

## split into different custers
```{r}
Cluster_0 <- filter_data %>% filter(Cluster == 0)
Cluster_1 <- filter_data %>% filter(Cluster == 1)
Cluster_2 <- filter_data %>% filter(Cluster == 2)
Cluster_3 <- filter_data %>% filter(Cluster == 3)

reviews_0 <- Cluster_0$review_text
reviews_1 <- Cluster_1$review_text
reviews_2 <- Cluster_2$review_text
reviews_3 <- Cluster_3$review_text
```

## -----Cluster 1-------------------------------------------
Load and pre-process txt files
```{r}
#break into individual sentences
reviews_1_sent <- sent_detect_nlp(reviews_1)

stop_words <- stopwords("SMART")

# pre-processing:
pp_reviews_1 <- reviews_1_sent %>%
  lapply(gsub, pattern = "[\r\n]", replacement = "") %>%
  lapply(gsub, pattern = "[[:digit:]]+", replacement = "") %>%
  lapply(gsub, pattern = "[â€™|Ä°|ã|©]", replacement = "") %>%
  str_replace_all("'", "") %>%
  str_replace_all("[[:punct:][:cntrl:]]", " ") %>%
  str_trim %>%
  str_to_lower() %>%
  lemmatize_strings() %>%
  lapply(gsub, pattern = "*\\b[[:alpha:]]{1}\\b*", replacement = "")

# tokenize on space and output as a list:
doc.list_1 <- str_split(pp_reviews_1, "[[:space:]]+")

# compute the table of term counts:
term.table_1 <- unlist(doc.list_1)
term.table_1 <- table(term.table_1[!term.table_1 %in% c("")])
term.table_1 <- sort(term.table_1, decreasing = TRUE)

# remove terms that are stop words or occur fewer than 5 times:
del_1 <- names(term.table_1) %in% stop_words | term.table_1 < 5
term.table_1 <- term.table_1[!del_1]
vocab_1 <- names(term.table_1)

# Place text reviews into the format required by the lda package:
get.terms <- function(x) {
index <- match(x, vocab_1)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents_1 <- lapply(doc.list_1, get.terms)

# Compute statistics of data set:
doc.length_1 <- sapply(documents_1, function(x) sum(x[2, ]))
term.frequency_1 <- as.integer(term.table_1)

```

Generate LDA model for cluster 1
```{r}
# MCMC and model tuning parameters:
K <- 6
G <- 7000
alpha <- 0.02
eta <- 0.02

# Fit the model:
set.seed(3101)
fit_1 <- lda.collapsed.gibbs.sampler(documents = documents_1, K = K, vocab = vocab_1, num.iterations = G, alpha = alpha, eta = eta, initial = NULL, burnin = 0, compute.log.likelihood = TRUE)

theta_1 <- t(apply(fit_1$document_sums + alpha, 2, function(x) x/sum(x)))
phi_1 <- t(apply(t(fit_1$topics) + eta, 2, function(x) x/sum(x)))
finalreview_1 <- list(phi = phi_1, theta = theta_1, doc.length = doc.length_1, vocab = vocab_1, term.frequency = term.frequency_1)

# create the JSON object to feed the visualization:
json_1 <- createJSON(phi = finalreview_1$phi, theta = finalreview_1$theta, doc.length = finalreview_1$doc.length, vocab = finalreview_1$vocab, term.frequency = finalreview_1$term.frequency)

serVis(json_1, out.dir = 'dsa3101_vis_cluster1', open.browser = FALSE)
```

Get top words for each topic
```{r}

#show top words for that topic
cluster1_top20 <- data.frame(top.topic.words(fit_1$topics, 20))
names(cluster1_top20) <- c("Location", "Food related", "Hotel amenities/ vicinity", "Room amenities", "General comments",   "Staff/ service")

cluster1_top20
```

## --Cluster 2-----------------------------------
Pre-process txt files
```{r}

#break into individual sentences
reviews_2_sent <- sent_detect_nlp(reviews_2)

# pre-processing:
pp_reviews_2 <- reviews_2_sent %>%
  lapply(gsub,pattern = "[\r\n]", replacement = "") %>%
  lapply(gsub, pattern = "[[:digit:]]+", replacement = "")%>%
  lapply(gsub, pattern = "[â€™|Ä°|ã|©]", replacement = "") %>%
  str_replace_all("'", "") %>%
  str_replace_all("[[:punct:][:cntrl:]]", " ") %>%
  str_trim %>%
  str_to_lower() %>%
  lemmatize_strings() %>%
  lapply(gsub, pattern = "*\\b[[:alpha:]]{1}\\b*", replacement = "")
  
# tokenize on space and output as a list:
doc.list_2 <- str_split(pp_reviews_2, "[[:space:]]+")

# compute the table of term counts:
term.table_2 <- unlist(doc.list_2)
term.table_2 <- table(term.table_2[!term.table_2 %in% c("")])
term.table_2 <- sort(term.table_2, decreasing = TRUE)

# remove terms that are stop words or occur fewer than 5 times:
del_2 <- names(term.table_2) %in% stop_words | term.table_2 < 5
term.table_2 <- term.table_2[!del_2]
vocab_2 <- names(term.table_2)

# Place text reviews into the format required by the lda package:
get.terms <- function(x) {
index <- match(x, vocab_2)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents_2 <- lapply(doc.list_2, get.terms)

# Compute statistics of data set:
doc.length_2 <- sapply(documents_2, function(x) sum(x[2, ]))
term.frequency_2 <- as.integer(term.table_2)

```

Generate LDA model for cluster 2
```{r}
# MCMC and model tuning parameters:
K <- 6
G <- 10000
alpha <- 0.02
eta <- 0.02

# Fit the model:
set.seed(3101)
fit_2 <- lda.collapsed.gibbs.sampler(documents = documents_2, K = K, vocab = vocab_2, num.iterations = G, alpha = alpha, eta = eta, initial = NULL, burnin = 0, compute.log.likelihood = TRUE)

theta_2 <- t(apply(fit_2$document_sums + alpha, 2, function(x) x/sum(x)))
phi_2 <- t(apply(t(fit_2$topics) + eta, 2, function(x) x/sum(x)))
finalreview_2 <- list(phi = phi_2, theta = theta_2, doc.length = doc.length_2, vocab = vocab_2, term.frequency = term.frequency_2)

# create the JSON object to feed the visualization:
json_2 <- createJSON(phi = finalreview_2$phi, theta = finalreview_2$theta, doc.length = finalreview_2$doc.length, vocab = finalreview_2$vocab, term.frequency = finalreview_2$term.frequency)

serVis(json_2, out.dir = 'dsa3101_vis_cluster2', open.browser = FALSE)
```

Get top words for each topic
```{r}
#show top words for that topic
cluster2_top20 <- data.frame(top.topic.words(fit_2$topics, 20))
names(cluster2_top20) <- c("Food related", "Staff/ service", "Location", "Hotel amenities/ vicinity", "Room amenities",  "General comments")

cluster2_top20

```

## --Cluster 3-----------------------------------
Pre-process txt files
```{r}

#break into individual sentences
reviews_3_sent <- sent_detect_nlp(reviews_3)

# pre-processing:
pp_reviews_3 <- reviews_3_sent %>%
  lapply(gsub,pattern = "[\r\n]", replacement = "") %>%
  lapply(gsub, pattern = "[[:digit:]]+", replacement = "")%>%
  lapply(gsub, pattern = "[â€™|Ä°|ã|©]", replacement = "") %>%
  str_replace_all("'", "") %>%
  str_replace_all("[[:punct:][:cntrl:]]", " ") %>%
  str_trim %>%
  str_to_lower() %>%
  lemmatize_strings() %>%
  lapply(gsub, pattern = "*\\b[[:alpha:]]{1}\\b*", replacement = "")

# tokenize on space and output as a list:
doc.list_3 <- str_split(pp_reviews_3, "[[:space:]]+")

# compute the table of term counts:
term.table_3 <- unlist(doc.list_3)
term.table_3 <- table(term.table_3[!term.table_3 %in% c("")])
term.table_3 <- sort(term.table_3, decreasing = TRUE)

# remove terms that are stop words or occur fewer than 5 times:
del_3 <- names(term.table_3) %in% stop_words | term.table_3 < 5
term.table_3 <- term.table_3[!del_3]
vocab_3 <- names(term.table_3)

# Place text reviews into the format required by the lda package:
get.terms <- function(x) {
index <- match(x, vocab_3)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents_3 <- lapply(doc.list_3, get.terms)

# Compute statistics of data set:
doc.length_3 <- sapply(documents_3, function(x) sum(x[2, ]))
term.frequency_3 <- as.integer(term.table_3)

```

Generate LDA model for cluster 3
```{r}
# Markov Chain Monte Carlo(MCMC) and model tuning parameters:
K <- 6
G <- 3000
alpha <- 0.02
eta <- 0.02

# Fit the model:
set.seed(3101)
fit_3 <- lda.collapsed.gibbs.sampler(documents = documents_3, K = K, vocab = vocab_3, num.iterations = G, alpha = alpha, eta = eta, initial = NULL, burnin = 0, compute.log.likelihood = TRUE)

theta_3 <- t(apply(fit_3$document_sums + alpha, 2, function(x) x/sum(x)))
phi_3 <- t(apply(t(fit_3$topics) + eta, 2, function(x) x/sum(x)))
finalreview_3 <- list(phi = phi_3, theta = theta_3, doc.length = doc.length_3, vocab = vocab_3, term.frequency = term.frequency_3)

# create the JSON object to feed the visualization:
json_3 <- createJSON(phi = finalreview_3$phi, theta = finalreview_3$theta, doc.length = finalreview_3$doc.length, vocab = finalreview_3$vocab, term.frequency = finalreview_3$term.frequency)

serVis(json_3, out.dir = 'dsa3101_vis_cluster3', open.browser = FALSE)
```

top words for each topic
```{r}
#show top words for that topic
cluster3_top10 <- data.frame(top.topic.words(fit_3$topics, 10))
names(cluster3_top10) <- c("Hotel amenities/ vicinity", "General comments", "Staff/ service", "Room amenities", "Location", "Food related")

cluster3_top10

```

## --Cluster 0-----------------------------------
Pre-process txt files
```{r}

#break into individual sentences
reviews_0_sent <- sent_detect_nlp(reviews_0)

# pre-processing:
pp_reviews_0 <- reviews_0_sent %>%
  lapply(gsub,pattern = "[\r\n]", replacement = "") %>%
  lapply(gsub, pattern = "[[:digit:]]+", replacement = "")%>%
  lapply(gsub, pattern = "[â€™|Ä°|ã|©]", replacement = "") %>%
  str_replace_all("'", "") %>%
  str_replace_all("[[:punct:][:cntrl:]]", " ") %>%
  str_trim %>%
  str_to_lower() %>%
  lemmatize_strings() %>%
  lapply(gsub, pattern = "*\\b[[:alpha:]]{1}\\b*", replacement = "")

# tokenize on space and output as a list:
doc.list_0 <- str_split(pp_reviews_0, "[[:space:]]+")

# compute the table of term counts:
term.table_0 <- unlist(doc.list_0)
term.table_0 <- table(term.table_0[!term.table_0 %in% c("")])
term.table_0 <- sort(term.table_0, decreasing = TRUE)

# remove terms that are stop words or occur fewer than 5 times:
del_0 <- names(term.table_0) %in% stop_words | term.table_0 < 5
term.table_0 <- term.table_0[!del_0]
vocab_0 <- names(term.table_0)

# Place text reviews into the format required by the lda package:
get.terms <- function(x) {
index <- match(x, vocab_0)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents_0 <- lapply(doc.list_0, get.terms)

# Compute statistics of data set:
doc.length_0 <- sapply(documents_0, function(x) sum(x[2, ]))
term.frequency_0 <- as.integer(term.table_0)

```

Generate LDA model for cluster 0
```{r}
# Markov Chain Monte Carlo(MCMC) and model tuning parameters:
K <- 6
G <- 5000
alpha <- 0.02
eta <- 0.02

# Fit the model:
set.seed(3101)
fit_0 <- lda.collapsed.gibbs.sampler(documents = documents_0, K = K, vocab = vocab_0, num.iterations = G, alpha = alpha, eta = eta, initial = NULL, burnin = 0, compute.log.likelihood = TRUE)

theta_0 <- t(apply(fit_0$document_sums + alpha, 2, function(x) x/sum(x)))
phi_0 <- t(apply(t(fit_0$topics) + eta, 2, function(x) x/sum(x)))
finalreview_0 <- list(phi = phi_0, theta = theta_0, doc.length = doc.length_0, vocab = vocab_0, term.frequency = term.frequency_0)

# create the JSON object to feed the visualization:
json_0 <- createJSON(phi = finalreview_0$phi, theta = finalreview_0$theta, doc.length = finalreview_0$doc.length, vocab = finalreview_0$vocab, term.frequency = finalreview_0$term.frequency)

serVis(json_0, out.dir = 'dsa3101_vis_cluster0', open.browser = FALSE)
```

top words for each topic
```{r}
#show top words for that topic
cluster0_top20 <- data.frame(top.topic.words(fit_0$topics, 20))
names(cluster0_top20) <- c("Room amenities", "Hotel amenities/ vicinity", "Staff/ service", "General comments", "Food related", "Location")

cluster0_top20
```

Export data for further analysis using Python
```{r}
cluster_num_0 <- rep(0, length(pp_reviews_0))
cluster_num_1 <- rep(1, length(pp_reviews_1))
cluster_num_2 <- rep(2, length(pp_reviews_2))
cluster_num_3 <- rep(3, length(pp_reviews_3))
cluster_num <- c(cluster_num_0, cluster_num_1, cluster_num_2, cluster_num_3)

to_export_0 <- unlist(pp_reviews_0)
to_export_1 <- unlist(pp_reviews_1)
to_export_2 <- unlist(pp_reviews_2)
to_export_3 <- unlist(pp_reviews_3)
clean_text <- c(to_export_0,to_export_1, to_export_2, to_export_3)

to_export <- data.frame(clean_text, cluster_num)

write.csv(to_export,"C:\\Users\\gordo\\Desktop\\cleaned_data_R.csv", row.names = FALSE)
write.csv(cluster0_top20,"C:\\Users\\gordo\\Desktop\\cluster0_top20.csv", row.names = FALSE)
write.csv(cluster1_top20,"C:\\Users\\gordo\\Desktop\\cluster1_top20.csv", row.names = FALSE)
write.csv(cluster2_top20,"C:\\Users\\gordo\\Desktop\\cluster2_top20.csv", row.names = FALSE)
write.csv(cluster3_top10,"C:\\Users\\gordo\\Desktop\\cluster3_top10.csv", row.names = FALSE)

```

Visualisations + Statistics of data
```{r}
word_counts_0 <- sapply(strsplit(unlist(pp_reviews_0), " "), length)
avg_review_length_0 <- sum(word_counts_0)/ length(reviews_0)
term_freq_df_0 <- data.frame(term.table_0)
names(term_freq_df_0) <- c("term0", "count0")
attach(term_freq_df_0)
unique_terms_0 <-length(term_freq_df_0$count0)

word_counts_1 <- sapply(strsplit(unlist(pp_reviews_1), " "), length)
avg_review_length_1 <- sum(word_counts_1)/ length(reviews_1)
term_freq_df_1 <- data.frame(term.table_1)
names(term_freq_df_1) <- c("term1", "count1")
attach(term_freq_df_1)
unique_terms_1 <-length(term_freq_df_1$count1)

word_counts_2 <- sapply(strsplit(unlist(pp_reviews_2), " "), length)
avg_review_length_2 <- sum(word_counts_2)/ length(reviews_2)
term_freq_df_2 <- data.frame(term.table_2)
names(term_freq_df_2) <- c("term2", "count2")
attach(term_freq_df_2)
unique_terms_2 <-length(term_freq_df_2$count2)

word_counts_3 <- sapply(strsplit(unlist(pp_reviews_3), " "), length)
avg_review_length_3 <- sum(word_counts_3)/ length(reviews_3)
term_freq_df_3 <- data.frame(term.table_3)
names(term_freq_df_3) <- c("term3", "count3")
attach(term_freq_df_3)
unique_terms_3 <- length(term_freq_df_3$count3)

cluster0_stats <- c(length(reviews_0), unique_terms_0, avg_review_length_0)
cluster1_stats <- c(length(reviews_1), unique_terms_1, avg_review_length_1)
cluster2_stats <- c(length(reviews_2), unique_terms_2, avg_review_length_2)
cluster3_stats <- c(length(reviews_3), unique_terms_3, avg_review_length_3)
stats_df <- round(data.frame(cluster0_stats, cluster1_stats, cluster2_stats, cluster3_stats),2)
rownames(stats_df) <- c("Number of reviews", "Unique terms used for LDA", "Average review Length")
colnames(stats_df) <- c("Cluster 0", "Cluster 1", "Cluster 2", "Cluster 3")

par(mfrow=c(2,2))
barplot(count0, main = "Term Frequency for Cluster 0", ylab = "Frequency", xlab = "Individual Words")
barplot(count1, main = "Term Frequency for Cluster 1", ylab = "Frequency", xlab = "Individual Words")
barplot(count2, main = "Term Frequency for Cluster 2", ylab = "Frequency", xlab = "Individual Words")
barplot(count3, main = "Term Frequency for Cluster 3", ylab = "Frequency", xlab = "Individual Words")

wordcloud(words = term_freq_df_0$term0, freq = term_freq_df_0$count0, min.freq = 1, max.words=100, random.order=FALSE, rot.per=0.2, colors=brewer.pal(8, "Dark2"))

wordcloud(words = term_freq_df_1$term1, freq = term_freq_df_1$count1, min.freq = 1, max.words=100, random.order=FALSE, rot.per=0.2, colors=brewer.pal(8, "Dark2"))

wordcloud(words = term_freq_df_2$term2, freq = term_freq_df_2$count2, min.freq = 1, max.words=100, random.order=FALSE, rot.per=0.2, colors=brewer.pal(8, "Dark2"))

wordcloud(words = term_freq_df_3$term3, freq = term_freq_df_3$count3, min.freq = 1, max.words=100, random.order=FALSE, rot.per=0.2, colors=brewer.pal(8, "Dark2"))

```

Generate Histogram of review length
```{r}
hist0 <- reviews_0 %>%
  lapply(gsub,pattern = "[\r\n]", replacement = "") %>%
  lapply(gsub, pattern = "[[:digit:]]+", replacement = "")%>%
  lapply(gsub, pattern = "[â€™|Ä°|ã|©]", replacement = "") %>%
  str_replace_all("'", "") %>%
  str_replace_all("[[:punct:][:cntrl:]]", " ") %>%
  str_trim %>%
  str_to_lower() %>%
  lemmatize_strings() %>%
  lapply(gsub, pattern = "*\\b[[:alpha:]]{1}\\b*", replacement = "")

hist1 <- reviews_1 %>%
  lapply(gsub,pattern = "[\r\n]", replacement = "") %>%
  lapply(gsub, pattern = "[[:digit:]]+", replacement = "")%>%
  lapply(gsub, pattern = "[â€™|Ä°|ã|©]", replacement = "") %>%
  str_replace_all("'", "") %>%
  str_replace_all("[[:punct:][:cntrl:]]", " ") %>%
  str_trim %>%
  str_to_lower() %>%
  lemmatize_strings() %>%
  lapply(gsub, pattern = "*\\b[[:alpha:]]{1}\\b*", replacement = "")

hist2 <- reviews_2 %>%
  lapply(gsub,pattern = "[\r\n]", replacement = "") %>%
  lapply(gsub, pattern = "[[:digit:]]+", replacement = "")%>%
  lapply(gsub, pattern = "[â€™|Ä°|ã|©]", replacement = "") %>%
  str_replace_all("'", "") %>%
  str_replace_all("[[:punct:][:cntrl:]]", " ") %>%
  str_trim %>%
  str_to_lower() %>%
  lemmatize_strings() %>%
  lapply(gsub, pattern = "*\\b[[:alpha:]]{1}\\b*", replacement = "")

hist3 <- reviews_3 %>%
  lapply(gsub,pattern = "[\r\n]", replacement = "") %>%
  lapply(gsub, pattern = "[[:digit:]]+", replacement = "")%>%
  lapply(gsub, pattern = "[â€™|Ä°|ã|©]", replacement = "") %>%
  str_replace_all("'", "") %>%
  str_replace_all("[[:punct:][:cntrl:]]", " ") %>%
  str_trim %>%
  str_to_lower() %>%
  lemmatize_strings() %>%
  lapply(gsub, pattern = "*\\b[[:alpha:]]{1}\\b*", replacement = "")

list0 <- str_split(hist0, "[[:space:]]+")
list0 <- unlist(lapply(list0, length))

list1 <- str_split(hist1, "[[:space:]]+")
list1 <- unlist(lapply(list1, length))

list2 <- str_split(hist2, "[[:space:]]+")
list2 <- unlist(lapply(list2, length))

list3 <- str_split(hist3, "[[:space:]]+")
list3 <- unlist(lapply(list3, length))

par(mfrow=c(2,2))
hist(list0, breaks = max(list3), main = "Histogram of review length for cluster 0", xlab = "Review length")
hist(list1, breaks = max(list3), main = "Histogram of review length for cluster 1", xlab = "Review length")
hist(list2, breaks = max(list3), main = "Histogram of review length for cluster 2", xlab = "Review length")
hist(list3, breaks = max(list3), main = "Histogram of review length for cluster 3", xlab = "Review length")


```
Generate csv file for all frequencies of words in cluster 3
```{r}
# compute the table of term counts:
term.table_3_low <- unlist(doc.list_3)
term.table_3_low <- table(term.table_3_low[!term.table_3 %in% c("")])
term.table_3_low <- sort(term.table_3_low, decreasing = TRUE)

# remove terms that are stop words
del_3_low <- names(term.table_3_low) %in% stop_words
term.table_3_low <- term.table_3_low[!del_3_low]
vocab_3_low <- names(term.table_3_low)

term_freq_df_3_low <- data.frame(term.table_3_low)
names(term_freq_df_3_low) <- c("term3low", "count3low")
attach(term_freq_df_3_low)

write.csv(term_freq_df_3_low,"C:\\Users\\gordo\\Desktop\\cluster3_all.csv", row.names = FALSE)

```